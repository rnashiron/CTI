# TensorflowによるDeepMind's WaveNet論文の実装

[![Build Status](https://travis-ci.org/ibab/tensorflow-wavenet.svg?branch=master)](https://travis-ci.org/ibab/tensorflow-wavenet)

これはthe [WaveNet generative neural
network architecture](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)という論文を基にした、音声を実際に生成するためのTensorflowによる実装です。


<table style="border-collapse: collapse">
<tr>
<td>
<p>
WaveNetneural network architectureとは、生の音の波形を生成し、文章から音声への変換や音声の作成に役立つつよいやつです。（詳しくは論文を読んで。）
</p>
<p>
Networkは、それまでの音声サンプルとモデルに関するパラメータから、次の音波サンプルを条件付確率の形で生成します。（多分、予測分布をつくる、くらいの意味）
</p>
<p>
音声の前処理が済んだら、インプットされた波形は整数の範囲で数値化されます。
その整数化された（音の？）振幅は、それからone-hot形式（1つだけHigh(1)であり、他はLow(0)であるようなビット列）でエンコードされ、<code>(num_samples, num_channels)</code>で表されるtensorを生成します。
</p>
<p>
現時点もしくは以前のインプットにのみアクセスする畳み込み層（A convolutional layer）は チャンネルの次元を減らします。
</p>
<p>
Networkのコアは、大量の<em>causal dilated layers</em>で構成され、その一つ一つがdilated convolution (convolution with holes)であり、そして現時点と以前の音声サンプルにのみアクセスする。
</p>
<p>
 すべての層でのアウトプットは結合され、元のチャンネル数に戻ります。その際、アウトプットをcategorical distributionに変換するようなソフトマックス関数が用いられます。（多分）
</p>
<p>
The loss function is the cross-entropy between the output for each timestep and the input at the next timestep.
損失関数は、各期のアウトプットとその次期のインプット間の交差エントロピーです。
</p>
<p>
このリポジトリでは、 ネットワークの実装は<a href="./wavenet/model.py">model.py</a>で書かれています。
(model.pyはwavenetフォルダ内にある)
</p>

  >追記:この辺の関数については https://qiita.com/shunchan0677/items/d30e5206677f2068a468, https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a あたりに書いてあるかもしれんと思った

</td>
<td width="300">
<img src="images/network.png" width="300"></img>
</td>
</tr>
</table>

## 必要なやつ

training scriptを回す前に、まずTensorflowをインストールしておいてください。
コードはTensorFlow ver. 1.0.1(Python 2.7, 3.5)でテストしています。

加えて、音声データの読み書きを可能にするために [librosa](https://github.com/librosa/librosa) をインストールする必要があります。

必要なパッケージをインストールするために、
```bash
pip install -r requirements.txt
```
を実行してください。
GPUサポートが必要であれば、
```bash
pip install -r requirements_gpu.txt
```
を使ってください。


>Anacondaを使う場合
```bash
$ conda list
```
>で既にインストールされてるパッケージを確認(指定したパッケージがあるか確認する方法しゃっちょが教えてくれた気がするけどわすれた)

>tensorflowないのでインストール
```bash
$ conda install tensorflow
```
>そしたらなんかいろんなパッケージをアップデートやらダウンロードやらするとか言い出したので任せる（すごい時間かかる）


## ネットワークの学習(Training the network)

 `.wav` 形式ファイルのコーパスを好きに使うことができます。
開発チームは主に [VCTK corpus](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) (around 10.4GB, [Alternative host](http://www.udialogue.org/download/cstr-vctk-corpus.html)) を使ってきました。

ネットワークを学習するために、
```bash
python train.py --data_dir=corpus
```
を実行してください。`corpus` というのは、`.wav` ファイルの入ったデイレクトリです。
このスクリプトは、ディレクトリ内のすべての`.wav`ファイルを再帰的に読み取ってくれるでしょう。

また、
```bash
python train.py --help
```
と実行することで、学習(training)の設定についての文書を見ることができます。

[`wavenet_params.json`](./wavenet_params.json)内の、モデルのパラメータの構造を見ることができます。
これら（のパラメータ）は、トレーニングと生成（のモデル）で同じである必要があります。

### Global Conditioning

Global conditioningというのは、.wavファイルの学習と生成とで互いに矛盾するようなもの（ID）を修正することです。

VCTK(様々な英語アクセントをもつ音声コーパスのこと)の場合、この‘‘ID"というのはスピーカーの整数IDをさし、100以上あります。（発話者は109人いるらしい）
In the case of the VCTK, this id is the integer id of the speaker, of which there are over a hundred.
音声を生成するときは、どの話者を模倣するのか、IDを指定することができます（実際は指定しなきゃいけない）。
詳細は論文かソースコードを読んでください。

### Training with Global Conditioning
学習に関する上記の説明(Training the network)は、global conditioningを使用しない場合のものです。global conditioningを用いて学習をする場合は、以下のようにコマンドライン引数を定めてください。
```
python train.py --data_dir=corpus --gc_channels=32
```
--gc_channels 引数は、以下の2つのことをやってくれます。
・global conditioningを含んだモデルにtrain.py scriptを渡す
・話者のID上の（話者についての、くらいの意味？）埋め込みベクトル(embedding vector)のサイズを特定化する

train.pyとaudio_reader.py内のglobal conditioningのロジックは、このVCTKコーパスと強く結びついており、VCYK内で使われるファイル名のパターンからスピーカーIDが決定できるものと期待されます。しかし、変更するのは簡単です。

## 音声の生成(Generating audio)

[Example output](https://soundcloud.com/user-731806733/tensorflow-wavenet-500-msec-88k-train-steps)
generated by @jyegerlehner based on speaker 280 from the VCTK corpus.

`generate.py` scriptを使うと、学習モデルに基づいて音声を生成できます。

### Global Conditioningを使わない場合

```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
を実行してください。`logdir/train/2017-02-13T16-45-34/model.ckpt-80000` は、前までで保存されたモデルへのパスとして必要です。(without extension)

`--samples` はどれだけの音声サンプルを生成したいかを表すパラメータです。 (デフォルトでは、16000の部分は1秒あたりの回数に対応する).

生成された波形は、TensorBoardを使って再生するか、`--wav_out_path`パラメータを使って`.wav` ファイルとして保存することができます。
```
python generate.py --wav_out_path=generated.wav --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
としてください。

 `--wav_out_path` に加えて`--save_every`を通すと、nサンプル毎の生成中wabファイルを保存することができます。
```
python generate.py --wav_out_path=generated.wav --save_every 2000 --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```

Fast generation がデフォルトで使用されるようになっています。これは [Fast Wavenet]リポジトリ(https://github.com/tomlepaine/fast-wavenet) から使用しています。
どのように動くかはリンク先の説明を読んでください。サンプルの生成に必要な時間を数分短縮することができます。

fast generationを無効にする場合は
```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000 --fast_generation=false
```
を実行してください。


### Global Conditioningを使用する場合
global conditioningを組み込んだモデルで音声を生成するときは、
```
python generate.py --samples 16000  --wav_out_path speaker311.wav --gc_channels=32 --gc_cardinality=377 --gc_id=311 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
としてください。
`--gc_channels=32` では、埋め込みベクトル(the embedding vector)のサイズが32であると定めており、これは学習する際に指定したサイズと一致させてください。

`--gc_cardinality=377` では、VCTKコーパスでの話者のIDの最大数が376であることを要求しています。もしほかのコーパスを使用する場合、この数字は、学習を行う際に自動的に決定される数字（train.pyスプリプトに記されている）と一致しなければなりません。

`--gc_id=311` は話者のIDを特定し、サンプルが生成されます。

## 動作テスト(Running tests)

test requirementsをインストールしてください。
```
pip install -r requirements_test.txt
```

test suiteをインストールしてください。
```
./ci/test.sh
```

## Missing features

Currently there is no local conditioning on extra information which would allow
context stacks or controlling what speech is generated.


## 関連するプロジェクト
- [tex-wavenet](https://github.com/Zeta36/tensorflow-tex-wavenet), a WaveNet を用いた文章生成
- [image-wavenet](https://github.com/Zeta36/tensorflow-image-wavenet), a WaveNet を用いた画像生成
