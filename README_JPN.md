# TensorflowによるDeepMind's WaveNet論文の実装

[![Build Status](https://travis-ci.org/ibab/tensorflow-wavenet.svg?branch=master)](https://travis-ci.org/ibab/tensorflow-wavenet)

これはthe [WaveNet generative neural
network architecture](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)という論文を基にした、音声を実際に生成するためのTensorflowによる実装です。


<table style="border-collapse: collapse">
<tr>
<td>
<p>
WaveNetneural network architectureとは、生の音の波形を生成し、文章から音声への変換や音声の作成に役立つつよいやつです。（詳しくは論文を読んで。）
</p>
<p>
Networkは、それまでの音声サンプルとモデルに関するパラメータから、次の音波サンプルを条件付確率の形で生成します。（多分、予測分布をつくる、くらいの意味）
</p>
<p>
音声の前処理が済んだら、インプットされた波形は整数の範囲で数値化されます。
その整数化された（音の？）振幅は、それからone-hot形式（1つだけHigh(1)であり、他はLow(0)であるようなビット列）でエンコードされ、<code>(num_samples, num_channels)</code>で表されるtensorを生成します。
</p>
<p>
現時点もしくは以前のインプットにのみアクセスする畳み込み層（A convolutional layer）は チャンネルの次元を減らします。
</p>
<p>
Networkのコアは、大量の<em>causal dilated layers</em>で構成され、その一つ一つがdilated convolution (convolution with holes)であり、そして現時点と以前の音声サンプルにのみアクセスする。
</p>
<p>
 すべての層でのアウトプットは結合され、元のチャンネル数に戻ります。その際、アウトプットをcategorical distributionに変換するようなソフトマックス関数が用いられます。（多分）
</p>
<p>
The loss function is the cross-entropy between the output for each timestep and the input at the next timestep.
損失関数は、各期のアウトプットとその次期のインプット間の交差エントロピーです。
</p>
<p>
In this repository, the network implementation can be found in <a href="./wavenet/model.py">model.py</a>.
…らしいけど、model.pyがなーーーーーい（404）
</p>
<p>
  追記:この辺の関数については https://qiita.com/shunchan0677/items/d30e5206677f2068a468 あたりに書いてあるかもしれんと思った
</p>
</td>
<td width="300">
<img src="images/network.png" width="300"></img>
</td>
</tr>
</table>

## 必要なやつ

training scriptを回す前に、まずTensorflowをインストールしておいてください。
コードはTensorFlow ver. 1.0.1(Python 2.7, 3.5)でテストしています。

加えて、音声データの読み書きを可能にするために [librosa](https://github.com/librosa/librosa) をインストールする必要があります。

必要なパッケージをインストールするために、
```bash
pip install -r requirements.txt
```
を実行してください。
GPUサポートが必要であれば、
```bash
pip install -r requirements_gpu.txt
```
を使ってください。

## ネットワークの学習(Training the network)

 `.wav` 形式ファイルのコーパスを好きに使うことができます。
開発チームは主に [VCTK corpus](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) (around 10.4GB, [Alternative host](http://www.udialogue.org/download/cstr-vctk-corpus.html)) を使ってきました。

ネットワークを学習するために、
```bash
python train.py --data_dir=corpus
```
を実行してください。`corpus` というのは、`.wav` ファイルの入ったデイレクトリです。
このスクリプトは、ディレクトリ内のすべての`.wav`ファイルを再帰的に読み取ってくれるでしょう。

また、
```bash
python train.py --help
```
と実行することで、学習(training)の設定についての文書を見ることができます。

[`wavenet_params.json`](./wavenet_params.json)内の、モデルのパラメータの構造を見ることができます。
これら（のパラメータ）は、トレーニングと生成（のモデル）で同じである必要があります。

### Global Conditioning
Global conditioning refers to modifying the model such that the id of a set of mutually-exclusive categories is specified during training and generation of .wav file.
Global conditioningというのは、.wav file.の学習と生成とで互いに矛盾するようなもの（ID）を修正することです。

VCTK(様々な英語アクセントをもつ音声コーパスのこと)の場合、この‘‘ID"というのはスピーカーの整数IDをさし、100以上あります。（発話者は109人いるらしい）
In the case of the VCTK, this id is the integer id of the speaker, of which there are over a hundred.
音声を生成するときは、どの話者を模倣するのか、IDを指定することができます（実際は指定しなきゃいけない）。
詳細は論文かソースコードを読んでください。

### Training with Global Conditioning
学習に関する上記の説明は、global conditioningがない場合でのものです。global conditioningを用いて学習をする場合は、以下のようにコマンドライン引数を定めてください。
```
python train.py --data_dir=corpus --gc_channels=32
```
--gc_channels 引数は、以下の2つのことをやってくれます。
・global conditioningを含んだモデルにtrain.py scriptを渡す
・話者のID上の（話者についての、くらいの意味？）埋め込みベクトル(embedding vector)のサイズを特定化する

train.pyとaudio_reader.py内のglobal conditioningのロジックは、このVCTKコーパスと強く結びついており、VCYK内で使われるファイル名のパターンからスピーカーIDが決定できるものと期待されます。しかし、変更は簡単に行われます。
The  logic in  is "hard-wired" to the VCTK corpus at the moment in that it expects to be able to determine the speaker id 
from the pattern of file naming used in VCTK, but can be easily be modified.

## Generating audio

[Example output](https://soundcloud.com/user-731806733/tensorflow-wavenet-500-msec-88k-train-steps)
generated by @jyegerlehner based on speaker 280 from the VCTK corpus.

You can use the `generate.py` script to generate audio using a previously trained model.

### Generating without Global Conditioning
Run
```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
where `logdir/train/2017-02-13T16-45-34/model.ckpt-80000` needs to be a path to previously saved model (without extension).
The `--samples` parameter specifies how many audio samples you would like to generate (16000 corresponds to 1 second by default).

The generated waveform can be played back using TensorBoard, or stored as a
`.wav` file by using the `--wav_out_path` parameter:
```
python generate.py --wav_out_path=generated.wav --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```

Passing `--save_every` in addition to `--wav_out_path` will save the in-progress wav file every n samples.
```
python generate.py --wav_out_path=generated.wav --save_every 2000 --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```

Fast generation is enabled by default.
It uses the implementation from the [Fast Wavenet](https://github.com/tomlepaine/fast-wavenet) repository.
You can follow the link for an explanation of how it works.
This reduces the time needed to generate samples to a few minutes.

To disable fast generation:
```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000 --fast_generation=false
```

### Generating with Global Conditioning
Generate from a model incorporating global conditioning as follows:
```
python generate.py --samples 16000  --wav_out_path speaker311.wav --gc_channels=32 --gc_cardinality=377 --gc_id=311 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
Where:

`--gc_channels=32` specifies 32 is the size of the embedding vector, and
must match what was specified when training.

`--gc_cardinality=377` is required
as 376 is the largest id of a speaker in the VCTK corpus. If some other corpus
is used, then this number should match what is automatically determined and
printed out by the train.py script at training time.

`--gc_id=311` specifies the id of speaker, speaker 311, for which a sample is
to be generated.

## Running tests

Install the test requirements
```
pip install -r requirements_test.txt
```

Run the test suite
```
./ci/test.sh
```

## Missing features

Currently there is no local conditioning on extra information which would allow
context stacks or controlling what speech is generated.


## 関連するプロジェクト
- [tex-wavenet](https://github.com/Zeta36/tensorflow-tex-wavenet), a WaveNet を用いた文章生成
- [image-wavenet](https://github.com/Zeta36/tensorflow-image-wavenet), a WaveNet を用いた画像生成
