# TensorflowによるDeepMind's WaveNet論文の実装

[![Build Status](https://travis-ci.org/ibab/tensorflow-wavenet.svg?branch=master)](https://travis-ci.org/ibab/tensorflow-wavenet)

これはthe [WaveNet generative neural
network architecture](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)という論文を基にした、音声を実際に生成するためのTensorflowによる実装です。


<table style="border-collapse: collapse">
<tr>
<td>
<p>
WaveNetneural network architectureとは、生の音の波形を生成し、文章から音声への変換や音声の作成に役立つつよいやつです。（詳しくは論文を読んで。）
</p>
<p>
Networkは、それまでの音声サンプルとモデルに関するパラメータから、次の音波サンプルを条件付確率の形で生成します。（多分、予測分布をつくる、くらいの意味）
</p>
<p>
音声の前処理が済んだら、インプットされた波形は整数の範囲で数値化されます。
その整数化された（音の？）振幅は、それからone-hot形式（1つだけHigh(1)であり、他はLow(0)であるようなビット列）でエンコードされ、<code>(num_samples, num_channels)</code>で表されるtensorを生成します。
</p>
<p>
現時点もしくは以前のインプットにのみアクセスする畳み込み層（A convolutional layer）は チャンネルの次元を減らします。
</p>
<p>
Networkのコアは、大量の<em>causal dilated layers</em>で構成され、その一つ一つがdilated convolution (convolution with holes)であり、そして現時点と以前の音声サンプルにのみアクセスする。
</p>
<p>
 すべての層でのアウトプットは結合され、元のチャンネル数に戻ります。その際、アウトプットをcategorical distributionに変換するようなソフトマックス関数が用いられます。（多分）
</p>
<p>
The loss function is the cross-entropy between the output for each timestep and the input at the next timestep.
損失関数は、各期のアウトプットとその次期のインプット間の交差エントロピーです。
</p>
<p>
In this repository, the network implementation can be found in <a href="./wavenet/model.py">model.py</a>.
</p>
<p>
  追記:この辺の関数については https://qiita.com/shunchan0677/items/d30e5206677f2068a468 あたりに書いてあるかもしれんと思った
</p>
</td>
<td width="300">
<img src="images/network.png" width="300"></img>
</td>
</tr>
</table>

## 必要なやつ

training scriptを回す前に、まずTensorflowをインストールしておいてください。
コードはTensorFlow ver. 1.0.1(Python 2.7, 3.5)でテストしています。

加えて、音声データの読み書きを可能にするために [librosa](https://github.com/librosa/librosa) をインストールする必要があります。

必要なパッケージをインストールするために、
```bash
pip install -r requirements.txt
```
を実行してください。
GPUサポートが必要であれば、
```bash
pip install -r requirements_gpu.txt
```
を使ってください。

## ネットワークの学習(Training the network)

 `.wav` 形式ファイルのコーパスを好きに使うことができます。
開発チームは主に [VCTK corpus](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) (around 10.4GB, [Alternative host](http://www.udialogue.org/download/cstr-vctk-corpus.html)) を使ってきました。

ネットワークを学習するために、
```bash
python train.py --data_dir=corpus
```
を実行してください。`corpus` というのは、`.wav` ファイルの入ったデイレクトリです。
このスクリプトは、ディレクトリ内のすべての`.wav`ファイルを再帰的に読み取ってくれるでしょう。

また、
```bash
python train.py --help
```
と実行することで、学習(training)の設定についての文書を見ることができます。

[`wavenet_params.json`](./wavenet_params.json)内の、モデルのパラメータの構造を見ることができます。
これら（のパラメータ）は、トレーニングと生成（のモデル）で同じである必要があります。

### Global Conditioning

Global conditioningというのは、.wavファイルの学習と生成とで互いに矛盾するようなもの（ID）を修正することです。

VCTK(様々な英語アクセントをもつ音声コーパスのこと)の場合、この‘‘ID"というのはスピーカーの整数IDをさし、100以上あります。（発話者は109人いるらしい）
In the case of the VCTK, this id is the integer id of the speaker, of which there are over a hundred.
音声を生成するときは、どの話者を模倣するのか、IDを指定することができます（実際は指定しなきゃいけない）。
詳細は論文かソースコードを読んでください。

### Training with Global Conditioning
学習に関する上記の説明は、global conditioningがない場合でのものです。global conditioningを用いて学習をする場合は、以下のようにコマンドライン引数を定めてください。
```
python train.py --data_dir=corpus --gc_channels=32
```
--gc_channels 引数は、以下の2つのことをやってくれます。
・global conditioningを含んだモデルにtrain.py scriptを渡す
・話者のID上の（話者についての、くらいの意味？）埋め込みベクトル(embedding vector)のサイズを特定化する

train.pyとaudio_reader.py内のglobal conditioningのロジックは、このVCTKコーパスと強く結びついており、VCYK内で使われるファイル名のパターンからスピーカーIDが決定できるものと期待されます。しかし、変更するのは簡単です。

## 音声の生成(Generating audio)

[Example output](https://soundcloud.com/user-731806733/tensorflow-wavenet-500-msec-88k-train-steps)
generated by @jyegerlehner based on speaker 280 from the VCTK corpus.

`generate.py` scriptを使うと、学習モデルに基づいて音声を生成できます。

### Global Conditioningを使わない場合

```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
を実行してください。`logdir/train/2017-02-13T16-45-34/model.ckpt-80000` は、前までで保存されたモデルへのパスとして必要です。(without extension)

`--samples` はどれだけの音声サンプルを生成したいかを表すパラメータです。 (デフォルトでは、16000の部分は1秒あたりの回数に対応する).

生成された波形は、TensorBoardを使って再生するか、`--wav_out_path`パラメータを使って`.wav` ファイルとして保存することができます。
```
python generate.py --wav_out_path=generated.wav --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
としてください。

 `--wav_out_path` に加えて`--save_every`を通すと、nサンプル毎の生成中wabファイルを保存することができます。
```
python generate.py --wav_out_path=generated.wav --save_every 2000 --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```

Fast generation がデフォルトで使用されるようになっています。これは [Fast Wavenet]リポジトリ(https://github.com/tomlepaine/fast-wavenet) から使用しています。
どのように動くかはリンク先の説明を読んでください。サンプルの生成に必要な時間を数分短縮することができます。

fast generationを無効にする場合は
```
python generate.py --samples 16000 logdir/train/2017-02-13T16-45-34/model.ckpt-80000 --fast_generation=false
```
を実行してください。


### Global Conditioningを使用する場合
Generate from a model incorporating global conditioning as follows:
```
python generate.py --samples 16000  --wav_out_path speaker311.wav --gc_channels=32 --gc_cardinality=377 --gc_id=311 logdir/train/2017-02-13T16-45-34/model.ckpt-80000
```
Where:

`--gc_channels=32` specifies 32 is the size of the embedding vector, and
must match what was specified when training.

`--gc_cardinality=377` is required
as 376 is the largest id of a speaker in the VCTK corpus. If some other corpus
is used, then this number should match what is automatically determined and
printed out by the train.py script at training time.

`--gc_id=311` specifies the id of speaker, speaker 311, for which a sample is
to be generated.

## Running tests

Install the test requirements
```
pip install -r requirements_test.txt
```

Run the test suite
```
./ci/test.sh
```

## Missing features

Currently there is no local conditioning on extra information which would allow
context stacks or controlling what speech is generated.


## 関連するプロジェクト
- [tex-wavenet](https://github.com/Zeta36/tensorflow-tex-wavenet), a WaveNet を用いた文章生成
- [image-wavenet](https://github.com/Zeta36/tensorflow-image-wavenet), a WaveNet を用いた画像生成
